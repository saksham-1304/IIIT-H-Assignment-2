# Neural Language Model Training - Pride and Prejudice

A from-scratch PyTorch implementation of a neural language model trained on Jane Austen's "Pride and Prejudice". This project demonstrates understanding of sequence models, training dynamics (underfitting/overfitting/best-fit), and model evaluation using perplexity.

## ğŸ“‹ Project Overview

- **Framework**: PyTorch (implemented from scratch, no pre-trained models)
- **Dataset**: Pride and Prejudice by Jane Austen (Project Gutenberg)
- **Model**: LSTM-based language model
- **Evaluation Metric**: Perplexity
- **Experiments**: Three scenarios demonstrating underfitting, overfitting, and best-fit

## ğŸ“ Trained Model Links

Due to file size limitations, trained model checkpoints are hosted on Google Drive:

**All Model Checkpoints**: [Download from Google Drive](https://drive.google.com/drive/folders/1xd95C6naTZhu_XOm7DZIK5rA-fceMrzD?usp=sharing)

This folder contains:
- Best-fit model checkpoints (best and final)
- Underfit model checkpoints (best and final)
- Overfit model checkpoints (best and final)



## ğŸ—‚ï¸ Project Structure

```
IIIT-H-Assignment-2/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessing.py      # Text preprocessing and tokenization
â”‚   â””â”€â”€ dataset.py            # PyTorch Dataset/DataLoader
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ lstm.py               # LSTM language model architecture
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config_underfit.json  # Underfitting configuration
â”‚   â”œâ”€â”€ config_bestfit.json   # Best-fit configuration
â”‚   â””â”€â”€ config_overfit.json   # Overfitting configuration
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ Pride_and_Prejudice-Jane_Austen.txt
â”œâ”€â”€ train.py                  # Training script
â”œâ”€â”€ evaluate.py               # Evaluation script
â”œâ”€â”€ generate_text.py          # Text generation script
â”œâ”€â”€ utils.py                  # Utility functions (plotting, checkpointing)
â”œâ”€â”€ run_all_experiments.py    # Run all experiments sequentially
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                 # This file

```

**Note**: After training, the following directories will be automatically generated:
- `checkpoints/` - Trained model files (`.pt` files)
- `plots/` - Loss and perplexity visualization plots (`.png` files)
- `outputs/` - Results and vocabulary JSON files

## ğŸš€ Setup Instructions

### Prerequisites

- Python 3.8 or higher
- CUDA-capable GPU (optional, but recommended)
- 8GB+ RAM

### Installation

1. **Clone the repository**

   ```bash
   git clone https://github.com/saksham-1304/IIIT-H-Assignment-2.git
   cd IIIT-H-Assignment-2
   ```

2. **Create a virtual environment (optional but recommended)**

   ```bash
   python -m venv venv
   
   # On Windows:
   .\venv\Scripts\activate
   
   # On Linux/Mac:
   source venv/bin/activate
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

## ğŸ¯ Running the Experiments

### Option 1: Run All Experiments at Once

```bash
python run_all_experiments.py
```

This will train all three models (underfit, bestfit, overfit) sequentially.

### Option 2: Run Individual Experiments

**Underfitting Experiment:**

```bash
python train.py --config configs/config_underfit.json --experiment underfit
```

**Best-fit Experiment:**

```bash
python train.py --config configs/config_bestfit.json --experiment bestfit
```

**Overfitting Experiment:**

```bash
python train.py --config configs/config_overfit.json --experiment overfit
```

## ğŸ“ˆ Evaluation

To evaluate a trained model:

```bash
python evaluate.py --checkpoint checkpoints/bestfit_best_model.pt --config configs/config_bestfit.json --experiment bestfit
```

This will:
- Calculate perplexity on train/val/test sets
- Generate sample text from the model

## ğŸ¨ Text Generation

To generate text using a trained model:

```bash
python generate_text.py --checkpoint checkpoints/bestfit_best_model.pt --config configs/config_bestfit.json --prompt "It is a truth"
```

## ğŸ“Š Experiment Configurations

### 1. Underfitting Configuration

- **Goal**: Demonstrate insufficient model capacity
- **Model**: Small (1 layer, 64 hidden units)
- **Training**: Short (10 epochs)
- **Regularization**: None
- **Expected**: High training and validation loss

### 2. Best-fit Configuration

- **Goal**: Achieve optimal generalization
- **Model**: Medium (2 layers, 256 hidden units)
- **Training**: Moderate (up to 50 epochs with early stopping)
- **Regularization**: Dropout (0.3), weight tying
- **Expected**: Low validation loss, good generalization

### 3. Overfitting Configuration

- **Goal**: Demonstrate overfitting behavior
- **Model**: Large (3 layers, 512 hidden units)
- **Training**: Long (100 epochs)
- **Regularization**: None
- **Expected**: Very low training loss, high validation loss

## ğŸ“ Output Files

After training, the following files will be generated:

### Checkpoints (in `checkpoints/`)

- `{experiment}_best_model.pt` - Best model based on validation loss
- `{experiment}_final_model.pt` - Final model after all epochs

### Plots (in `plots/`)

- `{experiment}_loss_plot.png` - Training vs validation loss curves
- `{experiment}_perplexity_plot.png` - Training vs validation perplexity curves

### Results (in `outputs/`)

- `{experiment}_results.json` - Detailed metrics and results
- `{experiment}_vocab.json` - Vocabulary mappings

## ğŸ”¬ Model Architecture

The LSTM language model consists of:

1. **Embedding Layer**: Converts token indices to dense vectors
2. **LSTM Layers**: Process sequences and capture dependencies
3. **Dropout**: Regularization technique (in best-fit model)
4. **Output Layer**: Projects to vocabulary size
5. **Weight Tying** (optional): Shares weights between embedding and output layers

### Model Parameters by Configuration

| Configuration | Embedding Dim | Hidden Dim | Layers | Dropout | Parameters |
|--------------|---------------|------------|--------|---------|------------|
| Underfit     | 64            | 64         | 1      | 0.0     | ~320K      |
| Best-fit     | 256           | 256        | 2      | 0.3     | ~5M        |
| Overfit      | 512           | 512        | 3      | 0.0     | ~20M       |

## ğŸ“Š Results Summary

### Actual Performance (Perplexity)

| Experiment | Train Perplexity | Val Perplexity | Test Perplexity | Status |
|-----------|------------------|----------------|-----------------|--------|
| Underfit  | 33.27            | 88.92          | 119.97          | âœ… High (underfitting) |
| **Best-fit** | **23.12**     | **75.66** â­   | **105.78** â­   | âœ… **Optimal** |
| Overfit   | 1.20             | 204.55         | 209.41          | âœ… Gap (overfitting) |

**Key Findings**:
- **Underfitting**: Both train and validation perplexities are high, indicating insufficient model capacity
- **Best-fit**: Lowest validation perplexity (75.66) with balanced train-val gap (52.54), demonstrating good generalization
- **Overfitting**: Extremely low training perplexity (1.20) but very high validation perplexity (204.55), showing memorization with poor generalization (gap: 203.35)

## ğŸ“ Key Learnings

1. **Underfitting**: Small model capacity leads to poor performance on both training and validation sets
2. **Best-fit**: Balanced model with proper regularization achieves good generalization
3. **Overfitting**: Large model without regularization memorizes training data but fails on validation

## ğŸ”§ Technical Details

### Reproducibility

- Fixed random seeds (PyTorch, NumPy, Python random)
- Deterministic CUDA operations
- Documented Python and package versions

### Training Features

- Gradient clipping to prevent exploding gradients
- Learning rate scheduling (ReduceLROnPlateau)
- Early stopping to prevent overfitting
- Checkpoint saving for best model

### Data Preprocessing

- Gutenberg metadata removal
- Word-level tokenization
- Vocabulary building (only on training data)
- 80/10/10 train/val/test split




## ğŸ› Troubleshooting

### Out of Memory Error

- Reduce `batch_size` in config files
- Use smaller `hidden_dim` or `embedding_dim`
- Reduce `seq_length`

### Slow Training

- Use GPU if available
- Increase `batch_size` (if memory allows)
- Reduce `num_epochs`

### Import Errors

- Ensure all dependencies are installed: `pip install -r requirements.txt`
- Check Python version: `python --version` (3.8+ required)

## ğŸ“š References

- [PyTorch Documentation](https://pytorch.org/docs/)
- [Pride and Prejudice (Project Gutenberg)](https://www.gutenberg.org/ebooks/42671)
- LSTM Paper: Hochreiter & Schmidhuber (1997)

## ğŸ‘¤ Author

**Saksham Singh Rathore**

- GitHub: [@saksham-1304](https://github.com/saksham-1304)
- Repository: [IIIT-H-Assignment-2](https://github.com/saksham-1304/IIIT-H-Assignment-2)

---

## âœ… Assignment Compliance

This project fully satisfies all requirements of Assignment 2: Neural Language Model Training (PyTorch)

### Core Requirements âœ…

- âœ… **Neural language model implemented from scratch** - Custom LSTM in `models/lstm.py` (151 lines)
- âœ… **Trained on provided dataset** - Pride and Prejudice (Project Gutenberg)
- âœ… **Training & validation loss plots** - 6 plots generated in `plots/` directory
- âœ… **Perplexity evaluation** - Computed on train/val/test splits (see Results Summary)
- âœ… **Multiple configurations compared** - 3 experiments with different architectures
- âœ… **Underfitting demonstrated** - Small model (1LÃ—64H, Val PPL: 88.92)
- âœ… **Overfitting demonstrated** - Large model (3LÃ—512H, Val PPL: 204.55)
- âœ… **Best-fit achieved** - Optimal model (2LÃ—256H, dropout 0.3, Val PPL: 75.66)

### Deliverables âœ…

- âœ… **Complete code** - 1500+ lines across 8 Python modules
- âœ… **Loss plots** - Training vs validation curves for all three scenarios
- âœ… **Perplexity metrics** - Final validation/test perplexity documented
- âœ… **Comprehensive report** - This README with setup, results, and analysis
- âœ… **Public GitHub repository** - All code accessible at [github.com/saksham-1304/IIIT-H-Assignment-2](https://github.com/saksham-1304/IIIT-H-Assignment-2)
- âœ… **Clear instructions** - Step-by-step setup and execution commands
- âœ… **Trained model links** - Google Drive folder with all checkpoints

### Rules Compliance âœ…

- âœ… **Only provided dataset used** - Pride and Prejudice exclusively
- âœ… **From-scratch implementation** - No pre-trained models, only PyTorch primitives
- âœ… **Fully reproducible** - Fixed seeds (seed=42), deterministic CUDA operations

### Extra Features (Bonus)

- âœ… Weight tying for parameter efficiency
- âœ… Gradient clipping to prevent exploding gradients
- âœ… Learning rate scheduling (ReduceLROnPlateau)
- âœ… Early stopping to optimize training
- âœ… Text generation with temperature sampling
- âœ… Batch experiment runner for automation
- âœ… Comprehensive logging and progress tracking
- âœ… Professional documentation with 340+ lines

---

## ğŸ“„ License

This project is for educational purposes as part of IIIT Hyderabad Assignment 2.

## ğŸ™ Acknowledgments

- IIIT Hyderabad for the assignment
- Jane Austen for Pride and Prejudice
- Project Gutenberg for making texts freely available
